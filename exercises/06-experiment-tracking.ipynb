{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking ðŸ“ˆ\n",
    "\n",
    "We can now try to improve our metrics in a structured manner.\n",
    "\n",
    "But it's important to keep track of our progress and that of our colleagues, rathing than doing it in an ad-hoc manner and eye-balling the outputs. This is where experiment tracking comes in.\n",
    "\n",
    "In this notebook, we'll track our experiments in the cloud, including inputs, prompts, outputs and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv add azureml-core\n",
    "import azureml.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()  # This load our .env file as environment variables\n",
    "\n",
    "SUBSCRIPTION_ID = os.environ[\"SUBSCRIPTION_ID\"]\n",
    "RESOURCE_GROUP = os.environ[\"RESOURCE_GROUP\"]\n",
    "WORKSPACE_NAME = os.environ[\"WORKSPACE_NAME\"]\n",
    "USER_NAME = os.environ[\"USER_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Azure ML Workspace\n",
    "ws = Workspace(subscription_id=SUBSCRIPTION_ID,\n",
    "               resource_group=RESOURCE_GROUP,\n",
    "               workspace_name=WORKSPACE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For autoreloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from llmops_training.news_reader.data import get_evaluation_data\n",
    "from llmops_training.news_reader.extraction import (\n",
    "    extract_general_info_from_articles, \n",
    "    get_general_info_prompt_template,\n",
    ")\n",
    "from llmops_training.news_reader.evaluation import (\n",
    "    evaluate_extract_general_info_success_rate, \n",
    "    evaluate_summarization, \n",
    "    evaluate_title,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = get_evaluation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating an Experiment in Azure ML. Under this experiment we will create and track multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name=f\"llmops-experiment-{USER_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start a new run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = USER_NAME + \"-\" + str(int(time.time()))\n",
    "run = experiment.start_logging(name=run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a part of the pipeline we want to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_info_prompt_template = get_general_info_prompt_template()\n",
    "general_info_list = extract_general_info_from_articles(\n",
    "    general_info_prompt_template, eval_data[\"article\"].to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the metrics with the generated outputs of the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'general_info_success_rate': 1.0,\n",
       " 'title_accuracy': 1.0,\n",
       " 'summarization_rouge_1': 0.3047572529066488}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    \"general_info_success_rate\": evaluate_extract_general_info_success_rate(general_info_list),\n",
    "    \"title_accuracy\": evaluate_title(general_info_list, eval_data),\n",
    "    \"summarization_rouge_1\": evaluate_summarization(general_info_list, eval_data)[\"rouge-1\"]\n",
    "}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log some metrics and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log(\"general_info_prompt_template\", general_info_prompt_template)\n",
    "for m in metrics:\n",
    "    run.log(m, metrics[m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And stop the run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Run through the above code if you haven't already.\n",
    "> - Explore the results in the Azure ML studio the Jobs section.\n",
    "> - Improve the prompt and try to get better results.\n",
    "> - Compare your results in the Azure ML Experiments section.\n",
    "> - (Bonus) Extend the `evaluation.py` script to log metrics and parameters to Azure ML Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy-llmops-in-azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
