{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the solution ðŸ’¯\n",
    "\n",
    "We have a working solution, but we don't know yet how often it is \"correct\".\n",
    "\n",
    "We can evaluate the output quality of the LLM in different ways, using both *offline* and *online* metrics. In our current situation, before we have brought the application to production, we are in the *offline* phase.\n",
    "\n",
    "In this notebook, we will explore different forms of evaluations in the offline phase.\n",
    "\n",
    "Some useful resources about LLM evals are [this blogpost](https://hamel.dev/blog/posts/evals/) and [this blogpost](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing basic functionality\n",
    "\n",
    "We might not always have labeled data available. In any case, unit tests can help you build confidence in the system you're shipping to production.\n",
    "\n",
    "We can test helper functions like we're already doing in `tests/test_extraction.py`. \n",
    "\n",
    "But we can also test the main functions that define our application, such as our `extract_general_info`, which extracts title and summary, or `extract_businesses_involved`, which extracts the businesses involved in the news article.\n",
    "\n",
    "For example, we can assert that the summary is no more than 2 sentences, or that the extracted businesses indeed occur in the article.\n",
    "\n",
    "What else could we test for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmops_training.news_reader.data import get_bbc_news_sample\n",
    "from llmops_training.news_reader.extraction import (\n",
    "    extract_general_info,\n",
    "    extract_businesses_involved,\n",
    "    get_general_info_prompt_template,\n",
    "    get_businesses_involved_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_bbc_news_sample()\n",
    "article = data[data[\"is_business\"]].iloc[0].article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"The US firm behind the Roomba smart vacuum cleaner, iRobot, has filed for bankruptcy protection after facing competition from Chinese rivals and being hit by tariffs.\n",
    "Under the so-called pre-packaged Chapter 11 process, the main manufacturer of its devices, Shenzhen-based Picea Robotics, will take ownership of the firm.\n",
    "The tough commercial landscape had forced iRobot to cut its prices and make major investments in new technology, according to documents filed on Sunday.\n",
    "US import duties of 46% on goods from Vietnam, where most of iRobot's devices for the American market are made, increased its costs by $23m (Â£17.2m) this year, the firm said.\n",
    "The loss-making company was valued at $3.56bn in 2021 after the pandemic helped to drive strong demand for its products. It is now valued at around $140m.\n",
    "On Friday, iRobot's shares fell by more than 13% on the technology-heavy Nasdaq trading platform in New York.\n",
    "iRobot said the bankruptcy filing was not expected to disrupt its app, supply chains or product support.\n",
    "Founded in 1990 by three members of the Massachusetts Institute of Technology's (MIT) Artificial Intelligence Lab, iRobot initially focused on defence and space technology before launching the Roomba in 2002.\n",
    "The Roomba holds about 42% of the US market share and 65% of the Japanese market share for robotic vacuum cleaners, according to the company.\n",
    "Last year, a planned $1.7bn takeover deal by online retail giant Amazon was derailed by the European Union's competition watchdog.\n",
    "Trade tariffs imposed by US Donald Trump on goods entering America from overseas has added to costs to many businesses, including iRobot, which rely on imports for product manufacturing.\n",
    "Trump has argued that the import taxes will boost American jobs and industry.\n",
    "Picea is a manufacturer of robotic vacuum cleaners, with research and development and production facilities in China and Vietnam.\n",
    "It has more than 7,000 employees worldwide and has sold more than 20 million robotic vacuum cleaners.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Fill in the TODO's in the test functions below.\n",
    "> - Run the tests in the subsequent cell\n",
    "> - Move these tests to the `tests/test_extraction.py` file\n",
    "> - Run the tests from the command line using `uv run pytest tests`\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summary_not_too_long(article: str):\n",
    "    summary = extract_general_info(get_general_info_prompt_template(), article).summary\n",
    "    summary_length = len(summary.split(\". \"))\n",
    "    assert summary_length <= 2, f\"Summary is too long: {summary_length} sentences\"\n",
    "\n",
    "\n",
    "def test_extracted_businesses_are_in_article(article: str):\n",
    "    businesses = extract_businesses_involved(\n",
    "        get_businesses_involved_prompt_template(), article\n",
    "    ).businesses\n",
    "    for business in businesses:\n",
    "        # TODO: Fill me in! Assert that the business is in the article\n",
    "        assert business.lower() in article.lower(), f\"Business '{business}' not found in the article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/evaluating-the-solution/unit-tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary_not_too_long(article)\n",
    "test_extracted_businesses_are_in_article(article)\n",
    "\n",
    "print(\"ðŸŽ‰ Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating using labeled data\n",
    "\n",
    "To evaluate your application in more depth in a structured manner, it is essential to collect labeled data for your use case.\n",
    "\n",
    "We can then define metrics, such as accuracy or ROUGE score, that we can iterate and improve upon.\n",
    "\n",
    "Luckily, we already have some labeled examples that we can use for evaluating. We only load a few in this training, so the evaluations remain cheap and go relatively quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from llmops_training.news_reader.data import get_evaluation_data\n",
    "from llmops_training.news_reader.extraction import (\n",
    "    GeneralInfo, \n",
    "    extract_business_category_from_articles,\n",
    "    get_business_category_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few articles for illustration\n",
    "eval_data = get_evaluation_data()\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate different aspects of our application, such as:\n",
    "- How many articles are correctly classified as *business* news or not\n",
    "- How many of the titles are correctly extracted\n",
    "- How many times the pipeline succeeds or errors\n",
    "- How well the summaries corresponds to the labeled summaries\n",
    "\n",
    "Let's implement the first one!\n",
    "\n",
    "First, we classify for each article if it is business news or not. We can then compare this to the labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_list = extract_business_category_from_articles(\n",
    "    get_business_category_prompt_template(),\n",
    "    eval_data[\"article\"].tolist()\n",
    ")\n",
    "business_category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Inspect the `evaluate_business_classification` function below.\n",
    "> - Fill in the TODO's in the function, so it accurately computes the accuracy of the business classification.\n",
    "> - Inspect the `evaluation` module in our package, which already has some evaluation functions implemented.\n",
    "> - Move this function to the `evaluation` module in our package.\n",
    "> - Uncomment the TODO's in the `evaluation` module to include this evaluation function.\n",
    "> - Run the `evaluation` module as script from the command line using:\n",
    ">   ```bash\n",
    ">   uv run python -m llmops_training.news_reader.evaluation\n",
    ">   ```\n",
    "> - Can you see the `business_classification_accuracy` in the output? ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_business_classification(\n",
    "    business_category_list: List[Optional[GeneralInfo]], data: pd.DataFrame\n",
    ") -> float:\n",
    "    \"\"\"Return accuracy of classification of whether an article is about business.\n",
    "\n",
    "    Data should contain:\n",
    "    - a column \"is_business\" with a boolean indicating whether the article is about business.\n",
    "\n",
    "    The provided list of business categories should be in the same order as the data.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i, row in data.iterrows():\n",
    "        if business_category_list[i] is None:\n",
    "            continue\n",
    "\n",
    "        is_about_business_prediction = business_category_list[i].is_about_business\n",
    "        is_about_business_labeled = row[\"is_business\"]\n",
    "        \n",
    "        incorrect += 1  # TODO: Extend me! Write if-else statement to increment correct or incorrect\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/evaluating-the-solution/evaluate-business-classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Making evaluations part of tests\n",
    "\n",
    "We can also include these evaluations in our test suite. For example, we can assert that the business classification accuracy is above a certain threshold. We can do so similarly for each metric.\n",
    "\n",
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Inspect the `test_run_evaluation` function below.\n",
    "> - Fill in the TODO's in the function, so it asserts that the metrics pass rates are satisfied.\n",
    "> - Move this function to the `tests/test_evaluation.py` file.\n",
    "> - Run the tests from the command line using `uv run pytest tests`\n",
    ">\n",
    "> ðŸ’¡ Hint:\n",
    ">\n",
    "> - Choose sensible pass rates for the metrics. Don't be too strict for now, or your tests will fail quite often.\n",
    "> - From the summarization metrics, focus just on `summarization_rouge_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmops_training.news_reader.evaluation import run_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_evaluation(evaluation_data: pd.DataFrame):\n",
    "    results = run_evaluation(evaluation_data)\n",
    "\n",
    "    # TODO: Fill me in! Choose pass rates for metrics and assert they are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/evaluating-the-solution/evaluation-threshold.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy-llmops-in-azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
