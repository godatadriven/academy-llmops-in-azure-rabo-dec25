{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the solution ðŸ’¯\n",
    "\n",
    "We have a working solution, but we don't know yet how often it is \"correct\".\n",
    "\n",
    "We can evaluate the output quality of the LLM in different ways, using both *offline* and *online* metrics. In our current situation, before we have brought the application to production, we are in the *offline* phase.\n",
    "\n",
    "In this notebook, we will explore different forms of evaluations in the offline phase.\n",
    "\n",
    "Some useful resources about LLM evals are [this blogpost](https://hamel.dev/blog/posts/evals/) and [this blogpost](https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit testing basic functionality\n",
    "\n",
    "We might not always have labeled data available. In any case, unit tests can help you build confidence in the system you're shipping to production.\n",
    "\n",
    "We can test helper functions like we're already doing in `tests/test_extraction.py`. \n",
    "\n",
    "But we can also test the main functions that define our application, such as our `extract_general_info`, which extracts title and summary, or `extract_businesses_involved`, which extracts the businesses involved in the news article.\n",
    "\n",
    "For example, we can assert that the summary is no more than 2 sentences, or that the extracted businesses indeed occur in the article.\n",
    "\n",
    "What else could we test for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmops_training.news_reader.data import get_bbc_news_sample\n",
    "from llmops_training.news_reader.extraction import (\n",
    "    extract_general_info,\n",
    "    extract_businesses_involved,\n",
    "    get_general_info_prompt_template,\n",
    "    get_businesses_involved_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "\n",
    "f = open('article.md', 'r')\n",
    "article=markdown.markdown( f.read() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Fill in the TODO's in the test functions below.\n",
    "> - Run the tests in the subsequent cell\n",
    "> - Move these tests to the `tests/test_extraction.py` file\n",
    "> - Run the tests from the command line using `uv run pytest tests`\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summary_not_too_long(article: str):\n",
    "    summary = extract_general_info(get_general_info_prompt_template(), article).summary\n",
    "    summary_length = len(summary.split(\". \"))\n",
    "    assert False, summary_length <= 2 # TODO: Fill me in! Assert that summary is <= 2 sentences\n",
    "\n",
    "\n",
    "def test_extracted_businesses_are_in_article(article: str):\n",
    "    businesses = extract_businesses_involved(\n",
    "        get_businesses_involved_prompt_template(), article\n",
    "    ).businesses\n",
    "    for business in businesses:\n",
    "        # TODO: Fill me in! Assert that the business is in the article\n",
    "        assert False, f\"Business '{business}' not found in the article\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/evaluating-the-solution/unit-tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_summary_not_too_long\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m test_extracted_businesses_are_in_article(article)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸŽ‰ Tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mtest_summary_not_too_long\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m      2\u001b[0m summary \u001b[38;5;241m=\u001b[39m extract_general_info(get_general_info_prompt_template(), article)\u001b[38;5;241m.\u001b[39msummary\n\u001b[1;32m      3\u001b[0m summary_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(summary\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, summary_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "test_summary_not_too_long(article)\n",
    "test_extracted_businesses_are_in_article(article)\n",
    "\n",
    "print(\"ðŸŽ‰ Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating using labeled data\n",
    "\n",
    "To evaluate your application in more depth in a structured manner, it is essential to collect labeled data for your use case.\n",
    "\n",
    "We can then define metrics, such as accuracy or ROUGE score, that we can iterate and improve upon.\n",
    "\n",
    "Luckily, we already have some labeled examples that we can use for evaluating. We only load a few in this training, so the evaluations remain cheap and go relatively quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from llmops_training.news_reader.data import get_evaluation_data\n",
    "from llmops_training.news_reader.extraction import (\n",
    "    GeneralInfo, \n",
    "    extract_business_category_from_articles,\n",
    "    get_business_category_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few articles for illustration\n",
    "eval_data = get_evaluation_data()\n",
    "eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate different aspects of our application, such as:\n",
    "- How many articles are correctly classified as *business* news or not\n",
    "- How many of the titles are correctly extracted\n",
    "- How many times the pipeline succeeds or errors\n",
    "- How well the summaries corresponds to the labeled summaries\n",
    "\n",
    "Let's implement the first one!\n",
    "\n",
    "First, we classify for each article if it is business news or not. We can then compare this to the labeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_category_list = extract_business_category_from_articles(\n",
    "    get_business_category_prompt_template(),\n",
    "    eval_data[\"article\"].tolist()\n",
    ")\n",
    "business_category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Inspect the `evaluate_business_classification` function below.\n",
    "> - Fill in the TODO's in the function, so it accurately computes the accuracy of the business classification.\n",
    "> - Inspect the `evaluation` module in our package, which already has some evaluation functions implemented.\n",
    "> - Move this function to the `evaluation` module in our package.\n",
    "> - Uncomment the TODO's in the `evaluation` module to include this evaluation function.\n",
    "> - Run the `evaluation` module as script from the command line using:\n",
    ">   ```bash\n",
    ">   uv run python -m llmops_training.news_reader.evaluation\n",
    ">   ```\n",
    "> - Can you see the `business_classification_accuracy` in the output? ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_business_classification(\n",
    "    business_category_list: List[Optional[GeneralInfo]], data: pd.DataFrame\n",
    ") -> float:\n",
    "    \"\"\"Return accuracy of classification of whether an article is about business.\n",
    "\n",
    "    Data should contain:\n",
    "    - a column \"is_business\" with a boolean indicating whether the article is about business.\n",
    "\n",
    "    The provided list of business categories should be in the same order as the data.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i, row in data.iterrows():\n",
    "        if business_category_list[i] is None:\n",
    "            continue\n",
    "\n",
    "        is_about_business_prediction = business_category_list[i].is_about_business\n",
    "        is_about_business_labeled = row[\"is_business\"]\n",
    "        \n",
    "        incorrect += 1  # TODO: Extend me! Write if-else statement to increment correct or incorrect\n",
    "\n",
    "    accuracy = correct / (correct + incorrect)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/evaluating-the-solution/evaluate-business-classification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus) Making evaluations part of tests\n",
    "\n",
    "We can also include these evaluations in our test suite. For example, we can assert that the business classification accuracy is above a certain threshold. We can do so similarly for each metric.\n",
    "\n",
    "> **Exercise** ðŸ“\n",
    ">\n",
    "> - Inspect the `test_run_evaluation` function below.\n",
    "> - Fill in the TODO's in the function, so it asserts that the metrics pass rates are satisfied.\n",
    "> - Move this function to the `tests/test_evaluation.py` file.\n",
    "> - Run the tests from the command line using `uv run pytest tests`\n",
    ">\n",
    "> ðŸ’¡ Hint:\n",
    ">\n",
    "> - Choose sensible pass rates for the metrics. Don't be too strict for now, or your tests will fail quite often.\n",
    "> - From the summarization metrics, focus just on `summarization_rouge_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmops_training.news_reader.evaluation import run_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run_evaluation(evaluation_data: pd.DataFrame):\n",
    "    results = run_evaluation(evaluation_data)\n",
    "\n",
    "    # TODO: Fill me in! Choose pass rates for metrics and assert they are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/evaluating-the-solution/evaluation-threshold.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academy-llmops-in-azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
